<!DOCTYPE html5>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta author="Clémence">
    <meta author="Sylvain">
    <meta author="Nahor">
    <meta charset="utf-8">
    <title>Chef-d'Œuvre du M2 IGAI 2019/2020</title>
    <link rel="stylesheet" type="text/css" href="siteWeb_fichiers/layout.css">
    <link href="siteWeb_fichiers/css.css" rel="stylesheet">
</head>


<body>
	<div class="topnav">
	<div class="maintitle"><a name="top" href="#top">Ré-identification de véhicule</a></div>
	<a href="#refs">RÉFÉRENCES ET LIVRABLES</a>
	<a href="#productions">PRODUCTIONS</a>
	<a href="#Pres">PRÉSENTATION</a>
	<a href="#banner">ACCUEIL</a>
 </div>

<main>

<div id="banner" class="banner">
</div>

<div class="maincontent">

<section id="Pres">
    <a name="Pres">
    <h3><span class="titlebox"><b>PRÉSENTATION</b></span></h3></a><br>
    <p>
	Dans le cadre du <a href="http://departement-informatique.univ-tlse3.fr/master-igai/" target="_blank">Master 2 Informatique Graphique et Analyse d'Images</a> de l'Université Paul Sabatier de Toulouse, l'équipe Re-Id s'est consacrée à ré-identifier des véhicules dans un ensemble de vidéos par apprentissage profond.
 Le principe général du Chef-d'Oeuvre consiste en l'implantation de
travaux de recherches proposés récemment. L'équipe a répondu au sujet proposé par
Alain Crouzil, enseignant chercheur à l'Institut de
Recherche dans l'équipe <a href="https://www.irit.fr/departement/signaux-images/tci/" target="_blank">Traitement et Compréhension d'Images</a> .
	</p>

	<p>
    La multiplication des caméras de surveillance permet de fournir de très nombreuses vidéos susceptibles de contenir des informations importantes
    pour les enquêtes liées à des activités criminelles ou terroristes. Mais l'analyse de ces vidéos, qui peuvent atteindre plusieurs milliers
     d'heures d'enregistrements, est une tâche difficile qui nécessite beaucoup de temps et d'attention de la part des enquêteurs.
     Il y a donc un besoin urgent d'outils pour faciliter leurs recherches.
    La ré-identification de véhicules et, surtout, la ré-identification de personnes sont des domaines d'application de la vision
     par ordinateur qui ont fait l'objet de nombreux travaux récents faisant appel aux techniques d'apprentissage profond.
     Liu et al. <a href="#refs">[1]</a> ont été les premiers à proposer l'utilisation d'un réseau de neurones convolutif pour la ré-identification de véhicules.
     Ils ont également constitué le jeu de données <a href="https://vehiclereid.github.io/VeRi/"target="_blank">VeRI</a> .
	</p>

	<p>
	La technique utilisé dans ce projet est la méthode FACT. Cette méthode combine des descripteurs de couleurs avec des descripteurs SIFT ainsi que
  les sorties d'une couche du réseau de neurones GoogleNet. Le projet est réalisé en Python en utilisant les bibliothèques OpenCv et Keras.
 Il peut-être récupéré <a href="https://github.com/SylvainDeker/Vehicle-ReIdentification.git" target="_blank">ici</a>.
	</p>

<table cellspacing="15">
    	<tbody><tr>
    		<td><h4></h4></td>
    		<td><h4>Quelques exemples du jeu de donnée. </h4></td>
    		<td><h4></h4></td>
    	</tr>
    	<tr>
    		<td><div class="zoom"><img src="siteWeb_fichiers/0001_c001_00016450_0.jpg" alt="Jaune" width="300"></a></div></td>
    		<td><div class="zoom"><img src="siteWeb_fichiers/0003_c001_00021480_0.jpg" alt="Rouge" width="300"></a></div></td>
    		<td><div class="zoom"><img src="siteWeb_fichiers/0011_c017_00088445_0.jpg" alt="Camion" width="300"></a></div></td>
    	</tr>

    </tbody></table>

</section>

<!-- Description détaillée -->

<section id="productions">
<a name="ombre">
    <h3><span class="titlebox"><b>PRODUCTIONS</b></span></h3></a><br>
	<h5><span class="titlebox2"><b>L'interface Utilisateur</b></span></h5>
<p>
  L'interface utilisateur a été construit avec <a href="https://pypi.org/project/PyQt5/" target="_blank">PyQt5</a> :
   il s'agit d'un module libre qui permet de lier le langage Python avec la bibliothèque Qt. L'interface à été conçu dans
   le but de servir de démonstrateur, un algorithme de ré-identification à pour but d'être utilisé sur un ensemble de vidéo
   en temps réel. Ce n'est pas le but de ce chef d'oeuvre.
</p>
	<h5><span class="titlebox2"><b>La méthode FACT</b></span></h5>
<!--
<p>
	La méthode <b>FACT</b> , pour <b>F</b>usion of <b>A</b>ttribute and
	<b>C</b>olors <b>F</b>eatures , a été proposé par ... pour répondre
	à la problématique de ré-identification de véhicule dans des
	enregistrements de vidéos sruveillance.
	...
</p>
-->
	<span class="titlebox3"><b>Les Descripteurs Sémantiques</b></span>
<p>
	Les descripteurs sémantiques sont les informations importantes d'une image, dans le cas de véhicule cela peut-être le nombre de siège
  ou la forme des phares. Pour extraires ces descripteurs nous avons utilisé le réseau de neurones
  convolutif GoogleNet en utilisant les poids du jeu de données
  <a href="http://www.image-net.org/" target="_blank">imageNet</a> auquel nous avons appliqué un réglage fin avec les données VeRi.
  Une fois l'entrainement du réseau terminé les descripteurs sémantiques sont obtenues en récupérant
   les valeurs de l'avant dernière couche du réseau de neurones lors de la classification de l'image.
</p>

	<span class="titlebox3"><b>Les Descripteurs de Couleurs</b></span>
<p>
  La méthode BOW-CN permet de calculer un descripteur de couleur sous la forme
  d’un vecteur obtenu par le calcul d’un histogramme sur 250 classes obtenues par un
  algorithme de k-moyenne. Les descripteurs permettant de calculer l’histogramme
  sont les valeurs des canaux des pixels.
</p>

	<span class="titlebox3"><b>Les Descripteurs de Textures</b></span>
<p>
  La méthode BOW-SIFT permet de calculer un descripteur de texture pour une image à partir
  d’un modèle d’apprentissage non supervisé de 10000 classes calculées à partir des descripteurs
  sift des points d’intérêts des images de la base d’apprentissage.
  On calcul alors un histogramme à 10000 classes qui compte le nombre de point d’intérêts
   prédit par classe pour une image, cet histogramme est le descripteur de texture BOW-SIFT.
</p>

	<span class="titlebox3"><b>Les Scores FACT</b></span>
<p>
  Le score FACT consiste à faire la somme pondérés des distances,
  entre l'image recherché et l'ensemble des images, des différents descripteurs.
  Les pondérations sont les suivantes :
  <ul>
    <li> 0,1 pour SIFT </li>
    <li> 0,2 pour CN </li>
    <li> 0,7 pour GoogleNet </li>
  </ul>
  Les parties les plus déterminantes de la ré-identification sont donc obtenues avec le réseau de neurones.
</p>

</div>
</main>


<!-- Auteurs, références et livrables -->
<footer id="refs">
    <table cellspacing="12">
        <tbody><tr>
            <td><h2>L'équipe</h2></td>
            <td><h2><b>Références</b></h2></td>
            <td><b>Phases</b></td>
            <td><b>Rapports</b></td>
        </tr>
        <tr>

            <td><b>Courdy-Bahsoun Clémence</b></td>
            <td><a href="https://ieeexplore.ieee.org/document/7553002" target="_blank">
                [1] X. Liu, W. Liu, H. Ma, H. Fu, <i>Large-scale vehicle re-identification in urban surveillance videos</i> (2016)</a></td>
            <td>Méthodes et Algorithmes</td>
            <td><a name="rapports" href="./siteWeb_fichiers/MethodeEtAlgorithme.pdf" target="_blank"><img src="siteWeb_fichiers/report.png" alt="Report logo" class="center" width="25"></a></td>
        </tr>
        <tr>
            <td><b>Deker Sylvain</b></td>
            <td><a href="https://arxiv.org/pdf/1804.02767" target="_blank">
                [2] J. Redmon, A. Farhadi, <i>YOLOv3: An Incremental Improvement</i> (2018)</a></td>
            <td>Spécifications</td>
            <td><a href="./siteWeb_fichiers/Specifications.pdf" target="_blank"><img src="siteWeb_fichiers/report.png" alt="Report logo" class="center" width="25"></a></td>
        </tr>
        <tr>
            <td><b>Moussa Nahor</b></td>
            <td><a href="https://arxiv.org/pdf/1506.08959" target="_blank">
                [3] L. Yang, P. Luo, C. C. Loy, X. Tang, <i>A Large-Scale Car Dataset for Fine-Grained Categorization and Verification</i> (2015)</a></td>
            <td>Conception</td>
            <td><a href="./siteWeb_fichiers/Conception_Re_id.pdf" target="_blank"><img src="siteWeb_fichiers/report.png" alt="Report logo" class="center" width="25"></a></td>
        </tr>
        <tr>
            <td><b></b></td>
            <td></td>
			<td>Recette</td>
            <td><a href="./siteWeb_fichiers/Recette.pdf" target="_blank"><img src="siteWeb_fichiers/report.png" alt="Report logo" class="center" width="25"></a></td>
        </tr>
        <tr>
            <td><b></b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
    </tbody></table>
</footer>




</body></html>
